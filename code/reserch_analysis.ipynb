{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6768a-7fa5-419d-b20e-d5a91465b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced plotting setup\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "print(\"🔬 ADVANCED HEATWAVE RESEARCH & PREDICTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading libraries and setting up advanced analysis environment...\")\n",
    "\n",
    "# Load all your processed data\n",
    "cpv_raw = pd.read_csv('/home/raj.ayush/results/heatwaves/cpv_new100.csv')\n",
    "print(f\"✅ Raw heatwave data loaded: {len(cpv_raw)} events\")\n",
    "\n",
    "# Load family data\n",
    "families_data = {}\n",
    "for fam_id in range(4):\n",
    "    try:\n",
    "        families_data[fam_id] = pd.read_csv(f'/home/raj.ayush/results/clustering_step1/cpv_fam{fam_id}.csv')\n",
    "        families_data[fam_id]['family_id'] = fam_id\n",
    "        print(f\"✅ Family {fam_id} loaded: {len(families_data[fam_id])} events\")\n",
    "    except:\n",
    "        print(f\"⚠️  Family {fam_id} not found\")\n",
    "\n",
    "print(\"🚀 Ready for advanced analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675aaa84-2f70-4451-8e8d-580ae33ca57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature engineering for prediction\n",
    "print(\"🔧 ADVANCED FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"Create advanced features for heatwave prediction\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Temporal features\n",
    "    df_features['time_amin'] = pd.to_datetime(df_features['time_amin'])\n",
    "    df_features['time_amax'] = pd.to_datetime(df_features['time_amax'])\n",
    "    \n",
    "    df_features['year'] = df_features['time_amin'].dt.year\n",
    "    df_features['month'] = df_features['time_amin'].dt.month\n",
    "    df_features['day_of_year'] = df_features['time_amin'].dt.dayofyear\n",
    "    df_features['season'] = ((df_features['month'] - 1) // 3) + 1\n",
    "    \n",
    "    # Cyclical encoding for seasonal patterns\n",
    "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['day_sin'] = np.sin(2 * np.pi * df_features['day_of_year'] / 365)\n",
    "    df_features['day_cos'] = np.cos(2 * np.pi * df_features['day_of_year'] / 365)\n",
    "    \n",
    "    # Spatial features\n",
    "    df_features['lat_lon_interaction'] = df_features['latitude_mean'] * df_features['longitude_mean']\n",
    "    df_features['spatial_distance_from_center'] = np.sqrt(\n",
    "        (df_features['latitude_mean'] - df_features['latitude_mean'].mean())**2 + \n",
    "        (df_features['longitude_mean'] - df_features['longitude_mean'].mean())**2\n",
    "    )\n",
    "    \n",
    "    # Advanced heatwave characteristics\n",
    "    df_features['intensity_per_day'] = df_features['HWMId_magnitude'] / (df_features['timespan'] + 1)\n",
    "    df_features['magnitude_log'] = np.log1p(df_features['HWMId_magnitude'])\n",
    "    df_features['duration_log'] = np.log1p(df_features['timespan'])\n",
    "    \n",
    "    # Statistical features\n",
    "    df_features['magnitude_zscore'] = (df_features['HWMId_magnitude'] - df_features['HWMId_magnitude'].mean()) / df_features['HWMId_magnitude'].std()\n",
    "    df_features['duration_zscore'] = (df_features['timespan'] - df_features['timespan'].mean()) / df_features['timespan'].std()\n",
    "    \n",
    "    # Temporal trends\n",
    "    df_features['years_since_start'] = df_features['year'] - df_features['year'].min()\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering to raw data\n",
    "cpv_enhanced = create_advanced_features(cpv_raw)\n",
    "\n",
    "# Filter clean data\n",
    "cpv_clean = cpv_enhanced[\n",
    "    (cpv_enhanced['timespan'] > 0) & \n",
    "    (cpv_enhanced['timespan'] <= 365) & \n",
    "    (cpv_enhanced['HWMId_magnitude'] > 0)\n",
    "].copy()\n",
    "\n",
    "print(f\"✅ Enhanced dataset created: {len(cpv_clean)} events with {len(cpv_clean.columns)} features\")\n",
    "\n",
    "# Display new features\n",
    "new_features = ['month_sin', 'month_cos', 'day_sin', 'day_cos', 'lat_lon_interaction', \n",
    "                'spatial_distance_from_center', 'intensity_per_day', 'magnitude_log', \n",
    "                'duration_log', 'magnitude_zscore', 'duration_zscore', 'years_since_start']\n",
    "\n",
    "print(\"🎯 New engineered features:\")\n",
    "for feat in new_features:\n",
    "    print(f\"   • {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99ffd7-a513-49d8-ae03-c9b460759c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced trend analysis\n",
    "print(\"📈 ADVANCED TREND ANALYSIS & FORECASTING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Yearly aggregation for trend analysis\n",
    "yearly_analysis = cpv_clean.groupby('year').agg({\n",
    "    'HWMId_magnitude': ['count', 'mean', 'std', 'max'],\n",
    "    'timespan': ['mean', 'std', 'max'],\n",
    "    'intensity_per_day': ['mean', 'std'],\n",
    "    'spatial_distance_from_center': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "yearly_analysis.columns = ['_'.join(col).strip() for col in yearly_analysis.columns]\n",
    "yearly_analysis = yearly_analysis.reset_index()\n",
    "\n",
    "print(\"📊 Yearly trend analysis:\")\n",
    "display(yearly_analysis.head())\n",
    "\n",
    "# Advanced trend visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('🔬 Advanced Heatwave Trend Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Frequency trend with confidence intervals\n",
    "years = yearly_analysis['year']\n",
    "event_counts = yearly_analysis['HWMId_magnitude_count']\n",
    "z = np.polyfit(years, event_counts, 1)\n",
    "p = np.poly1d(z)\n",
    "trend_line = p(years)\n",
    "\n",
    "axes[0,0].scatter(years, event_counts, alpha=0.7, s=60, color='darkblue')\n",
    "axes[0,0].plot(years, trend_line, '--', color='red', linewidth=2, label=f'Trend: {z[0]:+.2f} events/year')\n",
    "axes[0,0].fill_between(years, trend_line - np.std(event_counts - trend_line), \n",
    "                       trend_line + np.std(event_counts - trend_line), alpha=0.2, color='red')\n",
    "axes[0,0].set_title('📊 Heatwave Frequency Trend')\n",
    "axes[0,0].set_xlabel('Year')\n",
    "axes[0,0].set_ylabel('Number of Events')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Intensity evolution\n",
    "mean_intensity = yearly_analysis['HWMId_magnitude_mean']\n",
    "z2 = np.polyfit(years, mean_intensity, 1)\n",
    "p2 = np.poly1d(z2)\n",
    "\n",
    "axes[0,1].scatter(years, mean_intensity, alpha=0.7, s=60, color='darkred')\n",
    "axes[0,1].plot(years, p2(years), '--', color='orange', linewidth=2, label=f'Trend: {z2[0]:+.2f}/year')\n",
    "axes[0,1].set_title('🌡️  Mean Intensity Trend')\n",
    "axes[0,1].set_xlabel('Year')\n",
    "axes[0,1].set_ylabel('Mean Magnitude')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Duration evolution\n",
    "mean_duration = yearly_analysis['timespan_mean']\n",
    "z3 = np.polyfit(years, mean_duration, 1)\n",
    "p3 = np.poly1d(z3)\n",
    "\n",
    "axes[0,2].scatter(years, mean_duration, alpha=0.7, s=60, color='darkgreen')\n",
    "axes[0,2].plot(years, p3(years), '--', color='lime', linewidth=2, label=f'Trend: {z3[0]:+.2f} days/year')\n",
    "axes[0,2].set_title('⏱️  Mean Duration Trend')\n",
    "axes[0,2].set_xlabel('Year')\n",
    "axes[0,2].set_ylabel('Mean Duration (days)')\n",
    "axes[0,2].legend()\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Extreme events over time\n",
    "extreme_threshold = cpv_clean['HWMId_magnitude'].quantile(0.95)\n",
    "yearly_extremes = cpv_clean[cpv_clean['HWMId_magnitude'] >= extreme_threshold].groupby('year').size()\n",
    "all_years = range(cpv_clean['year'].min(), cpv_clean['year'].max() + 1)\n",
    "yearly_extremes = yearly_extremes.reindex(all_years, fill_value=0)\n",
    "\n",
    "axes[1,0].bar(yearly_extremes.index, yearly_extremes.values, alpha=0.7, color='purple')\n",
    "z4 = np.polyfit(yearly_extremes.index, yearly_extremes.values, 1)\n",
    "p4 = np.poly1d(z4)\n",
    "axes[1,0].plot(yearly_extremes.index, p4(yearly_extremes.index), '--', color='red', linewidth=2)\n",
    "axes[1,0].set_title('🚨 Extreme Events Trend (Top 5%)')\n",
    "axes[1,0].set_xlabel('Year')\n",
    "axes[1,0].set_ylabel('Number of Extreme Events')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Seasonal intensity patterns\n",
    "seasonal_intensity = cpv_clean.groupby('month')['HWMId_magnitude'].agg(['mean', 'std']).reset_index()\n",
    "months = seasonal_intensity['month']\n",
    "mean_vals = seasonal_intensity['mean']\n",
    "std_vals = seasonal_intensity['std']\n",
    "\n",
    "axes[1,1].errorbar(months, mean_vals, yerr=std_vals, marker='o', linewidth=2, capsize=5)\n",
    "axes[1,1].fill_between(months, mean_vals - std_vals, mean_vals + std_vals, alpha=0.3)\n",
    "axes[1,1].set_title('🗓️  Seasonal Intensity Patterns')\n",
    "axes[1,1].set_xlabel('Month')\n",
    "axes[1,1].set_ylabel('Mean Magnitude ± Std')\n",
    "axes[1,1].set_xticks(range(1, 13))\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Spatial-temporal correlation\n",
    "yearly_spatial = yearly_analysis[['year', 'spatial_distance_from_center_mean']]\n",
    "axes[1,2].scatter(yearly_spatial['year'], yearly_spatial['spatial_distance_from_center_mean'], \n",
    "                  alpha=0.7, s=60, color='brown')\n",
    "z5 = np.polyfit(yearly_spatial['year'], yearly_spatial['spatial_distance_from_center_mean'], 1)\n",
    "p5 = np.poly1d(z5)\n",
    "axes[1,2].plot(yearly_spatial['year'], p5(yearly_spatial['year']), '--', color='orange', linewidth=2)\n",
    "axes[1,2].set_title('🗺️  Spatial Dispersion Trend')\n",
    "axes[1,2].set_xlabel('Year')\n",
    "axes[1,2].set_ylabel('Mean Distance from Center')\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('/home/raj.ayush/results/final_ananananal/advanced_trend_analysis.png')\n",
    "plt.show()\n",
    "\n",
    "# Statistical significance of trends\n",
    "print(\"📈 TREND SIGNIFICANCE ANALYSIS:\")\n",
    "trends_data = [\n",
    "    ('Frequency', z[0], event_counts, years),\n",
    "    ('Intensity', z2[0], mean_intensity, years),\n",
    "    ('Duration', z3[0], mean_duration, years),\n",
    "    ('Extremes', z4[0], yearly_extremes.values, yearly_extremes.index)\n",
    "]\n",
    "\n",
    "for name, slope, y_data, x_data in trends_data:\n",
    "    correlation, p_value = stats.pearsonr(x_data, y_data)\n",
    "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "    print(f\"   • {name}: {slope:+.4f}/year, r={correlation:.3f}, p={p_value:.4f} {significance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183fe41-1615-485d-b60d-a31bc3c61440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced machine learning for heatwave prediction\n",
    "print(\"🤖 MACHINE LEARNING PREDICTION MODELS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Prepare data for ML\n",
    "feature_columns = [\n",
    "    'latitude_mean', 'longitude_mean', 'n_unique_g_ids', 'year', 'month', 'day_of_year',\n",
    "    'month_sin', 'month_cos', 'day_sin', 'day_cos', 'lat_lon_interaction',\n",
    "    'spatial_distance_from_center', 'years_since_start', 'season'\n",
    "]\n",
    "\n",
    "# Prediction tasks\n",
    "prediction_tasks = {\n",
    "    'Duration': 'timespan',\n",
    "    'Magnitude': 'HWMId_magnitude', \n",
    "    'Intensity': 'intensity_per_day'\n",
    "}\n",
    "\n",
    "ml_results = {}\n",
    "\n",
    "for task_name, target_col in prediction_tasks.items():\n",
    "    print(f\"\\n🎯 Predicting {task_name}...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = cpv_clean[feature_columns].copy()\n",
    "    y = cpv_clean[target_col].copy()\n",
    "    \n",
    "    # Handle any missing values\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Models to test\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "    }\n",
    "    \n",
    "    task_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        # Train model\n",
    "        if 'XGBoost' in model_name or 'Forest' in model_name or 'Gradient' in model_name:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        if 'XGBoost' in model_name or 'Forest' in model_name or 'Gradient' in model_name:\n",
    "            cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='r2').mean()\n",
    "        else:\n",
    "            cv_score = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2').mean()\n",
    "        \n",
    "        task_results[model_name] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R²': r2,\n",
    "            'CV_R²': cv_score,\n",
    "            'predictions': y_pred,\n",
    "            'actual': y_test\n",
    "        }\n",
    "        \n",
    "        print(f\"   {model_name:15}: R²={r2:.3f}, RMSE={rmse:.2f}, CV_R²={cv_score:.3f}\")\n",
    "    \n",
    "    ml_results[task_name] = task_results\n",
    "\n",
    "print(\"\\n✅ Machine learning models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a4f76-eb4c-4008-8ac5-cdd4cbc265d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ML model performance\n",
    "print(\"📊 MODEL PERFORMANCE VISUALIZATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
    "fig.suptitle('🤖 Machine Learning Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "for task_idx, (task_name, task_results) in enumerate(ml_results.items()):\n",
    "    # Performance metrics comparison\n",
    "    models = list(task_results.keys())\n",
    "    r2_scores = [task_results[model]['R²'] for model in models]\n",
    "    cv_scores = [task_results[model]['CV_R²'] for model in models]\n",
    "    rmse_scores = [task_results[model]['RMSE'] for model in models]\n",
    "    \n",
    "    # R² comparison\n",
    "    x_pos = np.arange(len(models))\n",
    "    axes[task_idx, 0].bar(x_pos - 0.2, r2_scores, 0.4, label='Test R²', alpha=0.8, color='skyblue')\n",
    "    axes[task_idx, 0].bar(x_pos + 0.2, cv_scores, 0.4, label='CV R²', alpha=0.8, color='orange')\n",
    "    axes[task_idx, 0].set_title(f'{task_name}: R² Score Comparison')\n",
    "    axes[task_idx, 0].set_ylabel('R² Score')\n",
    "    axes[task_idx, 0].set_xticks(x_pos)\n",
    "    axes[task_idx, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[task_idx, 0].legend()\n",
    "    axes[task_idx, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RMSE comparison\n",
    "    axes[task_idx, 1].bar(models, rmse_scores, alpha=0.8, color='lightcoral')\n",
    "    axes[task_idx, 1].set_title(f'{task_name}: RMSE Comparison')\n",
    "    axes[task_idx, 1].set_ylabel('RMSE')\n",
    "    axes[task_idx, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[task_idx, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Best model prediction vs actual\n",
    "    best_model = max(task_results.keys(), key=lambda x: task_results[x]['R²'])\n",
    "    y_pred = task_results[best_model]['predictions']\n",
    "    y_actual = task_results[best_model]['actual']\n",
    "    \n",
    "    axes[task_idx, 2].scatter(y_actual, y_pred, alpha=0.6, s=30)\n",
    "    # Perfect prediction line\n",
    "    min_val = min(min(y_actual), min(y_pred))\n",
    "    max_val = max(max(y_actual), max(y_pred))\n",
    "    axes[task_idx, 2].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[task_idx, 2].set_title(f'{task_name}: Best Model ({best_model})\\nR² = {task_results[best_model][\"R²\"]:.3f}')\n",
    "    axes[task_idx, 2].set_xlabel('Actual Values')\n",
    "    axes[task_idx, 2].set_ylabel('Predicted Values')\n",
    "    axes[task_idx, 2].legend()\n",
    "    axes[task_idx, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('/home/raj.ayush/results/final_ananananal/ml_model_performance.png')\n",
    "plt.show()\n",
    "\n",
    "# Performance summary table\n",
    "print(\"📈 MODEL PERFORMANCE SUMMARY TABLE\")\n",
    "performance_summary = []\n",
    "for task_name, task_results in ml_results.items():\n",
    "    for model_name, results in task_results.items():\n",
    "        performance_summary.append({\n",
    "            'Task': task_name,\n",
    "            'Model': model_name,\n",
    "            'R²': results['R²'],\n",
    "            'RMSE': results['RMSE'],\n",
    "            'MAE': results['MAE'],\n",
    "            'CV_R²': results['CV_R²']\n",
    "        })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_summary)\n",
    "print(\"\\n🏆 Best performing models:\")\n",
    "for task in prediction_tasks.keys():\n",
    "    task_df = performance_df[performance_df['Task'] == task]\n",
    "    best_model = task_df.loc[task_df['R²'].idxmax()]\n",
    "    print(f\"   {task:12}: {best_model['Model']:15} (R²={best_model['R²']:.3f}, RMSE={best_model['RMSE']:.2f})\")\n",
    "\n",
    "display(performance_df.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mt)",
   "language": "python",
   "name": "mt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
